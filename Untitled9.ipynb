{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJuoqMSlb1JKqaOEJhjqhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashgoyal-20/Cognitive-Computing-Assignments/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefcdVIqxa_z",
        "outputId": "33c61cf0-4c25-4a7c-ce22-43f786f3ee64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after lowercasing and punctuation removal:\n",
            " artificial intelligence ai is revolutionizing the world from selfdriving cars \n",
            "to intelligent virtual assistants ais applications are limitless it helps businesses automate tasks \n",
            "improve decisionmaking and enhance customer experiences ai algorithms process massive amounts of data in realtime \n",
            "making them invaluable in healthcare finance and education the future of technology is driven by advancements in ai\n",
            "\n",
            "Tokenized Words:\n",
            " ['artificial', 'intelligence', 'ai', 'is', 'revolutionizing', 'the', 'world', 'from', 'selfdriving', 'cars', 'to', 'intelligent', 'virtual', 'assistants', 'ais', 'applications', 'are', 'limitless', 'it', 'helps', 'businesses', 'automate', 'tasks', 'improve', 'decisionmaking', 'and', 'enhance', 'customer', 'experiences', 'ai', 'algorithms', 'process', 'massive', 'amounts', 'of', 'data', 'in', 'realtime', 'making', 'them', 'invaluable', 'in', 'healthcare', 'finance', 'and', 'education', 'the', 'future', 'of', 'technology', 'is', 'driven', 'by', 'advancements', 'in', 'ai']\n",
            "\n",
            "Tokenized Sentences:\n",
            " ['Artificial Intelligence (AI) is revolutionizing the world.', \"From self-driving cars \\nto intelligent virtual assistants, AI's applications are limitless!\", 'It helps businesses automate tasks, \\nimprove decision-making, and enhance customer experiences.', 'AI algorithms process massive amounts of data in real-time, \\nmaking them invaluable in healthcare, finance, and education.', 'The future of technology is driven by advancements in AI.']\n",
            "\n",
            "Words after Stopword Removal:\n",
            " ['artificial', 'intelligence', 'ai', 'revolutionizing', 'world', 'selfdriving', 'cars', 'intelligent', 'virtual', 'assistants', 'ais', 'applications', 'limitless', 'helps', 'businesses', 'automate', 'tasks', 'improve', 'decisionmaking', 'enhance', 'customer', 'experiences', 'ai', 'algorithms', 'process', 'massive', 'amounts', 'data', 'realtime', 'making', 'invaluable', 'healthcare', 'finance', 'education', 'future', 'technology', 'driven', 'advancements', 'ai']\n",
            "\n",
            "Word Frequency Distribution (excluding stopwords):\n",
            " Counter({'ai': 3, 'artificial': 1, 'intelligence': 1, 'revolutionizing': 1, 'world': 1, 'selfdriving': 1, 'cars': 1, 'intelligent': 1, 'virtual': 1, 'assistants': 1, 'ais': 1, 'applications': 1, 'limitless': 1, 'helps': 1, 'businesses': 1, 'automate': 1, 'tasks': 1, 'improve': 1, 'decisionmaking': 1, 'enhance': 1, 'customer': 1, 'experiences': 1, 'algorithms': 1, 'process': 1, 'massive': 1, 'amounts': 1, 'data': 1, 'realtime': 1, 'making': 1, 'invaluable': 1, 'healthcare': 1, 'finance': 1, 'education': 1, 'future': 1, 'technology': 1, 'driven': 1, 'advancements': 1})\n",
            "\n",
            "Porter Stemmer:\n",
            " ['artifici', 'intellig', 'ai', 'revolution', 'world', 'selfdriv', 'car', 'intellig', 'virtual', 'assist', 'ai', 'applic', 'limitless', 'help', 'busi', 'autom', 'task', 'improv', 'decisionmak', 'enhanc', 'custom', 'experi', 'ai', 'algorithm', 'process', 'massiv', 'amount', 'data', 'realtim', 'make', 'invalu', 'healthcar', 'financ', 'educ', 'futur', 'technolog', 'driven', 'advanc', 'ai']\n",
            "\n",
            "Lancaster Stemmer:\n",
            " ['art', 'intellig', 'ai', 'revolv', 'world', 'selfdr', 'car', 'intellig', 'virt', 'assist', 'ai', 'apply', 'limitless', 'help', 'busy', 'autom', 'task', 'improv', 'decisionmak', 'enh', 'custom', 'expery', 'ai', 'algorithm', 'process', 'mass', 'amount', 'dat', 'realtim', 'mak', 'invalu', 'healthc', 'fin', 'educ', 'fut', 'technolog', 'driv', 'adv', 'ai']\n",
            "\n",
            "Lemmatized Words:\n",
            " ['artificial', 'intelligence', 'ai', 'revolutionizing', 'world', 'selfdriving', 'car', 'intelligent', 'virtual', 'assistant', 'ai', 'application', 'limitless', 'help', 'business', 'automate', 'task', 'improve', 'decisionmaking', 'enhance', 'customer', 'experience', 'ai', 'algorithm', 'process', 'massive', 'amount', 'data', 'realtime', 'making', 'invaluable', 'healthcare', 'finance', 'education', 'future', 'technology', 'driven', 'advancement', 'ai']\n",
            "\n",
            "Words with more than 5 letters:\n",
            " ['Artificial', 'Intelligence', 'revolutionizing', 'driving', 'intelligent', 'virtual', 'assistants', 'applications', 'limitless', 'businesses', 'automate', 'improve', 'decision', 'making', 'enhance', 'customer', 'experiences', 'algorithms', 'process', 'massive', 'amounts', 'making', 'invaluable', 'healthcare', 'finance', 'education', 'future', 'technology', 'driven', 'advancements']\n",
            "\n",
            "Numbers extracted:\n",
            " []\n",
            "\n",
            "Capitalized Words:\n",
            " ['Artificial', 'Intelligence', 'From', 'It', 'The']\n",
            "\n",
            "Words with only alphabets:\n",
            " ['Artificial', 'Intelligence', 'AI', 'is', 'revolutionizing', 'the', 'world', 'From', 'self', 'driving', 'cars', 'to', 'intelligent', 'virtual', 'assistants', 'AI', 's', 'applications', 'are', 'limitless', 'It', 'helps', 'businesses', 'automate', 'tasks', 'improve', 'decision', 'making', 'and', 'enhance', 'customer', 'experiences', 'AI', 'algorithms', 'process', 'massive', 'amounts', 'of', 'data', 'in', 'real', 'time', 'making', 'them', 'invaluable', 'in', 'healthcare', 'finance', 'and', 'education', 'The', 'future', 'of', 'technology', 'is', 'driven', 'by', 'advancements', 'in', 'AI']\n",
            "\n",
            "Words starting with a vowel:\n",
            " ['Artificial', 'Intelligence', 'AI', 'is', 'intelligent', 'assistants', 'AI', 'applications', 'are', 'It', 'automate', 'improve', 'and', 'enhance', 'experiences', 'AI', 'algorithms', 'amounts', 'of', 'in', 'invaluable', 'in', 'and', 'education', 'of', 'is', 'advancements', 'in', 'AI']\n",
            "\n",
            "Custom Tokenized Words:\n",
            " ['Artificial', 'Intelligence', 'AI', 'is', 'revolutionizing', 'the', 'world', 'From', 'self-driving', 'cars', 'to', 'intelligent', 'virtual', 'assistants', 'AI', 's', 'applications', 'are', 'limitless', 'It', 'helps', 'businesses', 'automate', 'tasks', 'improve', 'decision-making', 'and', 'enhance', 'customer', 'experiences', 'AI', 'algorithms', 'process', 'massive', 'amounts', 'of', 'data', 'in', 'real-time', 'making', 'them', 'invaluable', 'in', 'healthcare', 'finance', 'and', 'education', 'The', 'future', 'of', 'technology', 'is', 'driven', 'by', 'advancements', 'in', 'AI']\n",
            "\n",
            "Text after regex replacements:\n",
            " Artificial Intelligence (AI) is revolutionizing the world. From self-driving cars \n",
            "to intelligent virtual assistants, AI's applications are limitless! It helps businesses automate tasks, \n",
            "improve decision-making, and enhance customer experiences. AI algorithms process massive amounts of data in real-time, \n",
            "making them invaluable in healthcare, finance, and education. The future of technology is driven by advancements in AI.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Make sure necessary NLTK packages are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Q1. Favorite Topic Paragraph\n",
        "paragraph = \"\"\"Artificial Intelligence (AI) is revolutionizing the world. From self-driving cars\n",
        "to intelligent virtual assistants, AI's applications are limitless! It helps businesses automate tasks,\n",
        "improve decision-making, and enhance customer experiences. AI algorithms process massive amounts of data in real-time,\n",
        "making them invaluable in healthcare, finance, and education. The future of technology is driven by advancements in AI.\"\"\"\n",
        "\n",
        "# 1. Convert text to lowercase and remove punctuation\n",
        "text_lower = paragraph.lower()\n",
        "text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Text after lowercasing and punctuation removal:\\n\", text_no_punct)\n",
        "\n",
        "# 2. Tokenize into words and sentences\n",
        "words = word_tokenize(text_no_punct)\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"\\nTokenized Words:\\n\", words)\n",
        "print(\"\\nTokenized Sentences:\\n\", sentences)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [w for w in words if w not in stop_words]\n",
        "print(\"\\nWords after Stopword Removal:\\n\", filtered_words)\n",
        "\n",
        "# 4. Word frequency distribution\n",
        "word_freq = Counter(filtered_words)\n",
        "print(\"\\nWord Frequency Distribution (excluding stopwords):\\n\", word_freq)\n",
        "\n",
        "# --------------------------------------\n",
        "\n",
        "# Q2: Stemming and Lemmatization\n",
        "ps = PorterStemmer()\n",
        "ls = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming\n",
        "porter_stemmed = [ps.stem(w) for w in filtered_words]\n",
        "lancaster_stemmed = [ls.stem(w) for w in filtered_words]\n",
        "print(\"\\nPorter Stemmer:\\n\", porter_stemmed)\n",
        "print(\"\\nLancaster Stemmer:\\n\", lancaster_stemmed)\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
        "print(\"\\nLemmatized Words:\\n\", lemmatized)\n",
        "\n",
        "# --------------------------------------\n",
        "\n",
        "# Q3: Regular Expressions and Text Splitting\n",
        "\n",
        "# a. Extract words with more than 5 letters\n",
        "words_5plus = re.findall(r'\\b\\w{6,}\\b', paragraph)\n",
        "print(\"\\nWords with more than 5 letters:\\n\", words_5plus)\n",
        "\n",
        "# b. Extract all numbers\n",
        "numbers = re.findall(r'\\b\\d+\\b', paragraph)\n",
        "print(\"\\nNumbers extracted:\\n\", numbers)\n",
        "\n",
        "# c. Extract all capitalized words\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', paragraph)\n",
        "print(\"\\nCapitalized Words:\\n\", capitalized_words)\n",
        "\n",
        "# 3a. Split text into words with alphabets only\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', paragraph)\n",
        "print(\"\\nWords with only alphabets:\\n\", alpha_words)\n",
        "\n",
        "# 3b. Extract words starting with a vowel\n",
        "vowel_words = [word for word in alpha_words if word.lower().startswith(('a', 'e', 'i', 'o', 'u'))]\n",
        "print(\"\\nWords starting with a vowel:\\n\", vowel_words)\n",
        "\n",
        "# --------------------------------------\n",
        "\n",
        "# Q4: Custom Tokenization & Regex-based Cleaning\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    # Remove punctuation except apostrophes in contractions and hyphens in hyphenated words\n",
        "    text = re.sub(r'[^\\w\\s\\'\\-\\.]', '', text)\n",
        "    # Tokenize words and numbers keeping decimals intact\n",
        "    tokens = re.findall(r'\\d+\\.\\d+|\\w+[-\\w]*|\\w+', text)\n",
        "    return tokens\n",
        "\n",
        "custom_tokens = custom_tokenizer(paragraph)\n",
        "print(\"\\nCustom Tokenized Words:\\n\", custom_tokens)\n",
        "\n",
        "# Regex Substitutions\n",
        "# a. Replace emails\n",
        "text_email = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', paragraph)\n",
        "# b. Replace URLs\n",
        "text_url = re.sub(r'http[s]?://\\S+|www\\.\\S+', '<URL>', text_email)\n",
        "# c. Replace phone numbers\n",
        "text_phone = re.sub(r'(\\+?\\d{1,3}[- ]?)?\\d{3}[- ]?\\d{3}[- ]?\\d{4}', '<PHONE>', text_url)\n",
        "\n",
        "print(\"\\nText after regex replacements:\\n\", text_phone)"
      ]
    }
  ]
}